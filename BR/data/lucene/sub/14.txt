266 chunks were cloned at a higher priority, and were all
restored to at least 2x replication within 2 minutes, thus
putting the cluster in a state where it could tolerate another
chunkserver failure without data loss.
6.3 Workload Breakdown
In this section, we present a detailed breakdown of the
workloads on two GFS clusters comparable but not identi-
cal to those in Section 6.2. Cluster X is for research and
development while cluster Y is for production data process-
ing.
6.3.1 Methodology and Caveats
These results include only client originated requests so
that they reflect the workload generated by our applications
for the file system as a whole. They do not include inter-
server requests to carry out client requests or internal back-
ground activities, such as forwarded writes or rebalancing.
Statistics on I/O operations are based on information
heuristically reconstructed from actual RPC requests logged
by GFS servers. For example, GFS client code may break a
read into multiple RPCs to increase parallelism, from which
we infer the original read. Since our access patterns are
highly stylized, we expect any error to be in the noise. Ex-
plicit logging by applications might have provided slightly
more accurate data, but it is logistically impossible to re-
compile and restart thousands of running clients to do so
and cumbersome to collect the results from as many ma-
chines.
One should be careful not to overly generalize from our
workload. Since Google completely controls both GFS and
its applications, the applications tend to be tuned for GFS,
and conversely GFS is designed for these applications. Such
mutual influence may also exist between general applications
Operation
Cluster
0K
1B..1K
1K..8K
8K..64K
64K..128K
128K..256K
256K..512K
512K..1M
1M..inf
Read
X Y
0.4 2.6
0.1 4.1
65.2 38.5
29.9 45.1
0.1 0.7
0.2 0.3
0.1 0.1
3.9 6.9
0.1 1.8
Write
X Y
0
0
6.6 4.9
0.4 1.0
17.8 43.0
2.3 1.9
31.6 0.4
4.2 7.7
35.5 28.7
1.5 12.3
Record Append
X
Y
0
0
0.2
9.2
18.9
15.2
78.0
2.8
< .1
4.3
< .1
10.6
< .1
31.2
2.2
25.5
0.7
2.2
Table 4: Operations Breakdown by Size (%). For
reads, the size is the amount of data actually read and trans-
ferred, rather than the amount requested.
and file systems, but the effect is likely more pronounced in
our case.
6.3.2 Chunkserver Workload
Table 4 shows the distribution of operations by size. Read
sizes exhibit a bimodal distribution. The small reads (un-
der 64 KB) come from seek-intensive clients that look up
small pieces of data within huge files. The large reads (over
512 KB) come from long sequential reads through entire
files.
A significant number of reads return no data at all in clus-
ter Y. Our applications, especially those in the production
systems, often use files as producer-consumer queues. Pro-
ducers append concurrently to a file while a consumer reads
the end of file. Occasionally, no data is returned when the
consumer outpaces the producers. Cluster X shows this less
often because it is usually used for short-lived data analysis
tasks rather than long-lived distributed applications.
Write sizes also exhibit a bimodal distribution. The large
writes (over 256 KB) typically result from significant buffer-
ing within the writers. Writers that buffer less data, check-
point or synchronize more often, or simply generate less data
account for the smaller writes (under 64 KB).
As for record appends, cluster Y sees a much higher per-
centage of large record appends than cluster X does because
our production systems, which use cluster Y, are more ag-
gressively tuned for GFS.
Table 5 shows the total amount of data transferred in op-
erations of various sizes. For all kinds of operations, the
larger operations (over 256 KB) generally account for most
of the bytes transferred. Small reads (under 64 KB) do
transfer a small but significant portion of the read data be-
cause of the random seek workload.
6.3.3 Appends versus Writes
Record appends are heavily used especially in our pro-
duction systems. For cluster X, the ratio of writes to record
appends is 108:1 by bytes transferred and 8:1 by operation
counts. For cluster Y, used by the production systems, the
ratios are 3.7:1 and 2.5:1 respectively. Moreover, these ra-
tios suggest that for both clusters record appends tend to
be larger than writes. For cluster X, however, the overall
usage of record append during the measured period is fairly
low and so the results are likely skewed by one or two appli-
cations with particular buffer size choices.
As expected, our data mutation workload is dominated
by appending rather than overwriting. We measured the
amount of data overwritten on primary replicas. This ap-
Operation
Cluster
1B..1K
1K..8K
8K..64K
64K..128K
128K..256K
256K..512K
512K..1M
1M..inf
Read
X
Y
< .1 < .1
13.8 3.9
11.4 9.3
0.3 0.7
0.8 0.6
1.4 0.3
65.9 55.1
6.4 30.1
Write
X
Y
< .1 < .1
< .1 < .1
2.4 5.9
0.3 0.3
16.5 0.2
3.4 7.7
74.1 58.0
3.3 28.0
Record Append
X
Y
< .1
< .1
